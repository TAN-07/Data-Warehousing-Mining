{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text_Classification.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOr4KLq3kBBI8znmTPz3JEU"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79NxquL5kc_A"
      },
      "source": [
        "#B.E. - B15 Tanvi Nirmal\n",
        "\n",
        "#**Experiment No. 04**\n",
        "#**Consider a suitable text dataset. Remove stop words, apply stemming and feature selection techniques to represent documents as vectors. Classify documents and evaluate precision, recall.**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "7cAB3vewQMYW"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfSoy2MBPf2n",
        "outputId": "be03cef0-a708-4bd0-9eab-ac80cb7d096d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(7613, 5)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method NDFrame.head of          id keyword  ...                                               text target\n",
              "0         1     NaN  ...  Our Deeds are the Reason of this #earthquake M...      1\n",
              "1         4     NaN  ...             Forest fire near La Ronge Sask. Canada      1\n",
              "2         5     NaN  ...  All residents asked to 'shelter in place' are ...      1\n",
              "3         6     NaN  ...  13,000 people receive #wildfires evacuation or...      1\n",
              "4         7     NaN  ...  Just got sent this photo from Ruby #Alaska as ...      1\n",
              "...     ...     ...  ...                                                ...    ...\n",
              "7608  10869     NaN  ...  Two giant cranes holding a bridge collapse int...      1\n",
              "7609  10870     NaN  ...  @aria_ahrary @TheTawniest The out of control w...      1\n",
              "7610  10871     NaN  ...  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...      1\n",
              "7611  10872     NaN  ...  Police investigating after an e-bike collided ...      1\n",
              "7612  10873     NaN  ...  The Latest: More Homes Razed by Northern Calif...      1\n",
              "\n",
              "[7613 rows x 5 columns]>"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ],
      "source": [
        "#dataset available at https://www.kaggle.com/c/nlp-getting-started/data\n",
        "#contains dataset for real and fake tweets on disasters\n",
        "\n",
        "dtf_train = pd.read_csv('/content/train.csv')\n",
        "print(dtf_train.shape)\n",
        "dtf_train.head"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Data Preprocessing**"
      ],
      "metadata": {
        "id": "vIF1pqVheMVU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dtf_train.isnull().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wzZ8wMUc7Gm",
        "outputId": "c8d9cc35-8bc1-41c8-f527-6a959a1d4212"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id             0\n",
              "keyword       61\n",
              "location    2533\n",
              "text           0\n",
              "target         0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#As most of the data in keyword and location columns is null and id is not needed, we can drop them.\n",
        "dtf_train = dtf_train.drop(['id','location','keyword'], axis=1)"
      ],
      "metadata": {
        "id": "sDotqkM1dQC_"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Performing basic NLP Techniques\n",
        "1. Removing unwanted words \n",
        "> *Using re to remove '#, =>, numbers, or ... etc' letters which are not required*\n",
        "2. Tokenizing data\n",
        "3. Transforming words to lowercase\n",
        "4. Stemming and removing stopwords\n",
        "> *Stop words : commonly words in english that may give misleading results.*\n",
        "> *Stemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form.*"
      ],
      "metadata": {
        "id": "wkSDG1pcekJt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing necessary libraries for data processing\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords               \n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.porter import PorterStemmer\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JBJToZ40dvu4",
        "outputId": "94d0617b-de17-436f-cb73-46b14a6f7422"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_unwanted(text):\n",
        "    un = re.compile(r'[^a-zA-Z]')\n",
        "    return re.sub(un,' ', text)\n",
        "\n",
        "dtf_train['clean_text'] = dtf_train['text'].apply(lambda x: remove_unwanted(x))  #Cleaning\n",
        "dtf_train['tokenized'] = dtf_train['clean_text'].apply(word_tokenize)            #Tokenization\n",
        "dtf_train['lower'] = dtf_train['tokenized'].apply(lambda x: [word.lower() for word in x])\n",
        "\n",
        "pst = PorterStemmer()\n",
        "dtf_train['no_stopwords'] = dtf_train['lower'].apply(lambda x: [pst.stem(word) for word in x])\n",
        "dtf_train['no_stopwords'] = dtf_train['no_stopwords'].apply(lambda x: [word for word in x if word not in set(stopwords.words('english'))])"
      ],
      "metadata": {
        "id": "dQCq6CsDRW1L"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dtf_train.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "2SAX94_vTY74",
        "outputId": "59bf0c00-65ef-4edd-9c97-7be85a6c8502"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "      <th>clean_text</th>\n",
              "      <th>tokenized</th>\n",
              "      <th>lower</th>\n",
              "      <th>no_stopwords</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
              "      <td>1</td>\n",
              "      <td>Our Deeds are the Reason of this  earthquake M...</td>\n",
              "      <td>[Our, Deeds, are, the, Reason, of, this, earth...</td>\n",
              "      <td>[our, deeds, are, the, reason, of, this, earth...</td>\n",
              "      <td>[deed, reason, thi, earthquak, may, allah, for...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
              "      <td>1</td>\n",
              "      <td>Forest fire near La Ronge Sask  Canada</td>\n",
              "      <td>[Forest, fire, near, La, Ronge, Sask, Canada]</td>\n",
              "      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n",
              "      <td>[forest, fire, near, la, rong, sask, canada]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>All residents asked to 'shelter in place' are ...</td>\n",
              "      <td>1</td>\n",
              "      <td>All residents asked to  shelter in place  are ...</td>\n",
              "      <td>[All, residents, asked, to, shelter, in, place...</td>\n",
              "      <td>[all, residents, asked, to, shelter, in, place...</td>\n",
              "      <td>[resid, ask, shelter, place, notifi, offic, ev...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
              "      <td>1</td>\n",
              "      <td>people receive  wildfires evacuation or...</td>\n",
              "      <td>[people, receive, wildfires, evacuation, order...</td>\n",
              "      <td>[people, receive, wildfires, evacuation, order...</td>\n",
              "      <td>[peopl, receiv, wildfir, evacu, order, califor...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
              "      <td>1</td>\n",
              "      <td>Just got sent this photo from Ruby  Alaska as ...</td>\n",
              "      <td>[Just, got, sent, this, photo, from, Ruby, Ala...</td>\n",
              "      <td>[just, got, sent, this, photo, from, ruby, ala...</td>\n",
              "      <td>[got, sent, thi, photo, rubi, alaska, smoke, w...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  ...                                       no_stopwords\n",
              "0  Our Deeds are the Reason of this #earthquake M...  ...  [deed, reason, thi, earthquak, may, allah, for...\n",
              "1             Forest fire near La Ronge Sask. Canada  ...       [forest, fire, near, la, rong, sask, canada]\n",
              "2  All residents asked to 'shelter in place' are ...  ...  [resid, ask, shelter, place, notifi, offic, ev...\n",
              "3  13,000 people receive #wildfires evacuation or...  ...  [peopl, receiv, wildfir, evacu, order, califor...\n",
              "4  Just got sent this photo from Ruby #Alaska as ...  ...  [got, sent, thi, photo, rubi, alaska, smoke, w...\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dtf_train['no_stopwords'] = [' '.join(map(str, l)) for l in dtf_train['no_stopwords']]"
      ],
      "metadata": {
        "id": "9HqchoX4TbQf"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dtf_train.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "nS325wcMW9ID",
        "outputId": "fb6828a1-6701-464d-b69c-c346532e9a3f"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "      <th>clean_text</th>\n",
              "      <th>tokenized</th>\n",
              "      <th>lower</th>\n",
              "      <th>no_stopwords</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
              "      <td>1</td>\n",
              "      <td>Our Deeds are the Reason of this  earthquake M...</td>\n",
              "      <td>[Our, Deeds, are, the, Reason, of, this, earth...</td>\n",
              "      <td>[our, deeds, are, the, reason, of, this, earth...</td>\n",
              "      <td>deed reason thi earthquak may allah forgiv us</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
              "      <td>1</td>\n",
              "      <td>Forest fire near La Ronge Sask  Canada</td>\n",
              "      <td>[Forest, fire, near, La, Ronge, Sask, Canada]</td>\n",
              "      <td>[forest, fire, near, la, ronge, sask, canada]</td>\n",
              "      <td>forest fire near la rong sask canada</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>All residents asked to 'shelter in place' are ...</td>\n",
              "      <td>1</td>\n",
              "      <td>All residents asked to  shelter in place  are ...</td>\n",
              "      <td>[All, residents, asked, to, shelter, in, place...</td>\n",
              "      <td>[all, residents, asked, to, shelter, in, place...</td>\n",
              "      <td>resid ask shelter place notifi offic evacu she...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
              "      <td>1</td>\n",
              "      <td>people receive  wildfires evacuation or...</td>\n",
              "      <td>[people, receive, wildfires, evacuation, order...</td>\n",
              "      <td>[people, receive, wildfires, evacuation, order...</td>\n",
              "      <td>peopl receiv wildfir evacu order california</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
              "      <td>1</td>\n",
              "      <td>Just got sent this photo from Ruby  Alaska as ...</td>\n",
              "      <td>[Just, got, sent, this, photo, from, Ruby, Ala...</td>\n",
              "      <td>[just, got, sent, this, photo, from, ruby, ala...</td>\n",
              "      <td>got sent thi photo rubi alaska smoke wildfir p...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  ...                                       no_stopwords\n",
              "0  Our Deeds are the Reason of this #earthquake M...  ...      deed reason thi earthquak may allah forgiv us\n",
              "1             Forest fire near La Ronge Sask. Canada  ...               forest fire near la rong sask canada\n",
              "2  All residents asked to 'shelter in place' are ...  ...  resid ask shelter place notifi offic evacu she...\n",
              "3  13,000 people receive #wildfires evacuation or...  ...        peopl receiv wildfir evacu order california\n",
              "4  Just got sent this photo from Ruby #Alaska as ...  ...  got sent thi photo rubi alaska smoke wildfir p...\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating a dictionary where key refers to words and value refers to the word count\n",
        "uniqueWordFrequents = {}\n",
        "for t in dtf_train['no_stopwords']:\n",
        "    for word in t.split():\n",
        "        if(word in uniqueWordFrequents.keys()):\n",
        "            uniqueWordFrequents[word] += 1\n",
        "        else:\n",
        "            uniqueWordFrequents[word] = 1\n",
        "            \n",
        "#Convert dictionary to dataFrame\n",
        "uniqueWordFrequents = pd.DataFrame.from_dict(uniqueWordFrequents,orient='index',columns=['Word Frequent'])\n",
        "uniqueWordFrequents.sort_values(by=['Word Frequent'], inplace=True, ascending=False)\n",
        "uniqueWordFrequents.head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "Z3cvtLlYW9K6",
        "outputId": "d6c4bd1a-4c41-4c3b-c2c8-7e0d80d13c78"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Word Frequent</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>co</th>\n",
              "      <td>4746</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>http</th>\n",
              "      <td>4721</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>thi</th>\n",
              "      <td>483</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>like</th>\n",
              "      <td>411</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>wa</th>\n",
              "      <td>395</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fire</th>\n",
              "      <td>363</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>amp</th>\n",
              "      <td>344</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>get</th>\n",
              "      <td>311</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ha</th>\n",
              "      <td>261</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bomb</th>\n",
              "      <td>239</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      Word Frequent\n",
              "co             4746\n",
              "http           4721\n",
              "thi             483\n",
              "like            411\n",
              "wa              395\n",
              "fire            363\n",
              "amp             344\n",
              "get             311\n",
              "ha              261\n",
              "bomb            239"
            ]
          },
          "metadata": {},
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Filtering out most frequent words\n",
        "uniqueWordFrequents = uniqueWordFrequents[uniqueWordFrequents['Word Frequent'] >= 20]\n",
        "uniqueWordFrequents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "aPfse9_AGrHb",
        "outputId": "4c212cec-631d-4bed-9509-71f0563e8dad"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Word Frequent</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>co</th>\n",
              "      <td>4746</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>http</th>\n",
              "      <td>4721</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>thi</th>\n",
              "      <td>483</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>like</th>\n",
              "      <td>411</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>wa</th>\n",
              "      <td>395</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>captur</th>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>polit</th>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>creat</th>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>radio</th>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>milit</th>\n",
              "      <td>20</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>800 rows Ã— 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        Word Frequent\n",
              "co               4746\n",
              "http             4721\n",
              "thi               483\n",
              "like              411\n",
              "wa                395\n",
              "...               ...\n",
              "captur             20\n",
              "polit              20\n",
              "creat              20\n",
              "radio              20\n",
              "milit              20\n",
              "\n",
              "[800 rows x 1 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Count Vectorization - creates a sparse matrix\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "countVec = CountVectorizer(max_features = uniqueWordFrequents.shape[0])\n",
        "bagOfWords = countVec.fit_transform(dtf_train['no_stopwords']).toarray()"
      ],
      "metadata": {
        "id": "lMuqT-xPGVk7"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bagOfWords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4OaqfRxSZoe4",
        "outputId": "f3e51138-7fee-458e-bc76-021cba79cca6"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       ...,\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [1, 0, 0, ..., 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = bagOfWords\n",
        "y = dtf_train['target']\n",
        "print(\"X shape = \",X.shape)\n",
        "print(\"y shape = \",y.shape)\n",
        "\n",
        "X_train , X_test , y_train , y_test = train_test_split(X,y,test_size=0.20, random_state=70, shuffle =True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4wDY53FIUPL",
        "outputId": "592c9e7e-a6dd-4aea-ece8-e6466435adf9"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X shape =  (7613, 800)\n",
            "y shape =  (7613,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Feature Selection**"
      ],
      "metadata": {
        "id": "5VRXfBxLbEdU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn import svm"
      ],
      "metadata": {
        "id": "T4QrjUPVaLyo"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Trying various feature selection algorithms\n",
        "LR = LogisticRegression()\n",
        "SVM = svm.SVC(kernel='linear')\n",
        "MNB = MultinomialNB()\n",
        "\n",
        "LR.fit(X_train,y_train)\n",
        "SVM.fit(X_train,y_train)\n",
        "MNB.fit(X_train,y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JtAR-r7SbIIw",
        "outputId": "b1f6eadc-77cd-4661-d3a6-4b9321b55877"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB()"
            ]
          },
          "metadata": {},
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictLR = LR.predict(X_test)\n",
        "predictSVM = SVM.predict(X_test)\n",
        "predictMNB = MNB.predict(X_test)"
      ],
      "metadata": {
        "id": "iS5JV2Debq1C"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Performance Analysis**"
      ],
      "metadata": {
        "id": "v9MPr_X0cMBq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(\"Logistic Regression : \\n\\n\"+classification_report(y_test,predictLR))\n",
        "print(\"\\n\\nSupport Vector Machine : \\n\\n\"+classification_report(y_test,predictSVM))\n",
        "print(\"\\n\\nMultinomial Naive Bayes : \\n\\n\"+classification_report(y_test,predictMNB))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eS6VBmRDb9lU",
        "outputId": "699196bf-1c7a-4f48-e4d5-7599d295fb35"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression : \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.86      0.83       875\n",
            "           1       0.80      0.73      0.76       648\n",
            "\n",
            "    accuracy                           0.80      1523\n",
            "   macro avg       0.80      0.79      0.80      1523\n",
            "weighted avg       0.80      0.80      0.80      1523\n",
            "\n",
            "\n",
            "\n",
            "Support Vector Machine : \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.86      0.83       875\n",
            "           1       0.80      0.72      0.75       648\n",
            "\n",
            "    accuracy                           0.80      1523\n",
            "   macro avg       0.80      0.79      0.79      1523\n",
            "weighted avg       0.80      0.80      0.80      1523\n",
            "\n",
            "\n",
            "\n",
            "Multinomial Naive Bayes : \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.82      0.81       875\n",
            "           1       0.75      0.73      0.74       648\n",
            "\n",
            "    accuracy                           0.78      1523\n",
            "   macro avg       0.78      0.77      0.77      1523\n",
            "weighted avg       0.78      0.78      0.78      1523\n",
            "\n"
          ]
        }
      ]
    }
  ]
}